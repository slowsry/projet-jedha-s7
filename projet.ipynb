{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "id keyword location  \\\n3258  10861     NaN      NaN   \n3259  10865     NaN      NaN   \n3260  10868     NaN      NaN   \n3261  10874     NaN      NaN   \n3262  10875     NaN      NaN   \n\n                                                   text  \n3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n3259  Storm in RI worse than last hurricane. My city...  \n3260  Green Line derailment in Chicago http://t.co/U...  \n3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n3262  #CityofCalgary has activated its Municipal Eme...  \n(3263, 4)\n(3237, 4)\n222\n"
    }
   ],
   "source": [
    "t = pd.read_csv('test.csv')\n",
    "print(t.tail())\n",
    "print(t.shape)\n",
    "\n",
    "print(t[t.keyword.isna() == False].shape)\n",
    "t.keyword.unique()\n",
    "print(len(t.keyword.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id  target\n0   0       0\n1   2       0\n2   3       0\n3   9       0\n4  11       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv('train.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7613, 4)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   keyword location                                               text  target\nid                                                                            \n1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7613, 4)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(7552, 4)\n222\n"
    }
   ],
   "source": [
    "print(ds[ds.keyword.isna() == False].shape)\n",
    "ds.keyword.unique()\n",
    "print(len(ds.keyword.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dropna(subset=['keyword'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(5080, 4)\n['Birmingham' 'Est. September 2012 - Bristol' 'AFRICA' 'Philadelphia, PA'\n 'London, UK' 'Pretoria' 'World Wide!!' nan 'Paranaque City'\n 'Live On Webcam' 'milky way' 'GREENSBORO,NORTH CAROLINA' 'England.'\n 'Sheffield Township, Ohio' 'India' 'Barbados' 'Anaheim' 'Abuja' 'USA'\n 'South Africa' 'Sao Paulo, Brazil' 'hollywoodland '\n 'Edmonton, Alberta - Treaty 6' 'Inang Pamantasan'\n 'Twitter Lockout in progress' 'Concord, CA' 'Calgary, AB' 'San Francisco'\n 'CLVLND' 'Nashville, TN' 'Santa Clara, CA' 'UK' 'St. Louis, MO'\n 'Walker County, Alabama' 'Australia' 'North Carolina' 'Norf Carolina'\n 'San Mateo County, CA' 'Njoro, Kenya' \"Your Sister's Bedroom\"\n 'Arlington, TX' 'South Bloomfield, OH' 'New Hanover County, NC'\n 'Maldives' 'Manchester, NH' 'Wilmington, NC' 'global'\n 'Alberta | Sask. | Montana' 'Charlotte' 'Baton Rouge, LA'\n 'Hagerstown, MD' 'Gloucestershire , UK' 'Nairobi, Kenya'\n 'Instagram - @heyimginog ' '304' 'Switzerland' 'US'\n 'Somewhere Only We Know ?' 'Belgium' 'dope show' 'Oshawa, Canada'\n 'Baker City Oregon' 'United States' 'marysville ca ' 'Hermosa Beach, CA'\n '19.600858, -99.047821' 'Pennsylvania' 'Salt Lake City, Utah'\n 'Palo Alto, CA' 'Spain but Opa-Locka, FL' 'Jaipur, India'\n 'Hyderabad Telangana INDIA' 'Eagle Pass, Texas' 'bangalore'\n 'Financial News and Views' 'Indonesia' 'y(our) boyfriends legs '\n 'New Mexico, USA' 'Somewhere Out There' 'Mumbai india' 'sri lanka'\n 'Not a U.S resident' 'Lehigh Valley, PA' 'Canada' 'Thrissur' 'Havenford'\n '92' 'Israel' 'Fashion Heaven. IG: TMId_' 'San Francisco, CA' 'italy'\n 'nyc' 'Toronto' 'Jackson' 'New York / Worldwide' 'New Orleans, LA'\n 'West Wales' 'Happily Married with 2 kids ' 'Cambridge, MA' 'Arizona ']\n3342\n"
    }
   ],
   "source": [
    "print(ds[ds.location.isna() == False].shape)\n",
    "print(ds.location.unique()[:100])\n",
    "print(len(ds.location.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.location.fillna(value='nd.', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "keyword  location  text\ntarget                         \n0          1439      1439  1439\n1          1033      1033  1033\n\n        keyword  location  text\ntarget                         \n0          2884      2884  2884\n1          2196      2196  2196\n"
    }
   ],
   "source": [
    "print(ds[ds.location=='nd.'].groupby('target').count())\n",
    "print()\n",
    "print(ds[ds.location!='nd.'].groupby('target').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.drop('location', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      keyword                                               text  target\n7578  wrecked   @jt_ruff23 @cameronhacker and I wrecked you both       0\n7579  wrecked  Three days off from work and they've pretty mu...       0\n7580  wrecked  #FX #forex #trading Cramer: Iger's 3 words tha...       0\n7581  wrecked  @engineshed Great atmosphere at the British Li...       0\n7582  wrecked  Cramer: Iger's 3 words that wrecked Disney's s...       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7578</th>\n      <td>wrecked</td>\n      <td>@jt_ruff23 @cameronhacker and I wrecked you both</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7579</th>\n      <td>wrecked</td>\n      <td>Three days off from work and they've pretty mu...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7580</th>\n      <td>wrecked</td>\n      <td>#FX #forex #trading Cramer: Iger's 3 words tha...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7581</th>\n      <td>wrecked</td>\n      <td>@engineshed Great atmosphere at the British Li...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7582</th>\n      <td>wrecked</td>\n      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "ds.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting en_core_web_sm==2.0.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4 MB)\nBuilding wheels for collected packages: en-core-web-sm\n  Building wheel for en-core-web-sm (setup.py): started\n  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.0.0-py3-none-any.whl size=37405980 sha256=0475987606d566b8bb578c09d8131b2812ea3c658ecd7560f4c0faf29ad5252a\n  Stored in directory: C:\\Users\\samuel\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-p41edwdq\\wheels\\51\\df\\77\\250d8a622c7fc066a42ea4238279337e4a5e04c2602c448ea5\nSuccessfully built en-core-web-sm\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-2.0.0\n\n\u001b[93m    Error: Couldn't link model to 'en_core_web_sm'\u001b[0m\n    Creating a symlink in spacy/data failed. Make sure you have the required\n    permissions and try re-running the command as admin, or use a\n    virtualenv. You can still import the model as a module and call its\n    load() method, or create the symlink manually.\n\n    C:\\Anaconda\\lib\\site-packages\\en_core_web_sm -->\n    C:\\Anaconda\\lib\\site-packages\\spacy\\data\\en_core_web_sm\n\n\n\u001b[93m    Creating a shortcut link for 'en' didn't work (maybe you don't have\n    admin permissions?), but you can still load the model via its full\n    package name: nlp = spacy.load('{name}')\u001b[0m\n    Download successful but linking failed\n\n"
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1 Oil and Gas Exploration Takes Seismic Shift in Gabon to Somalia - Bloomberg http://t.co/bEKrPjnYHs #??????? #Somalia\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['oil',\n 'gas',\n 'exploration',\n 'seismic',\n 'shift',\n 'gabon',\n 'somalia',\n '-',\n 'bloomberg',\n 'http://t.co/bekrpjnyhs',\n '#',\n '?',\n '?',\n '?',\n '?',\n '?',\n '?',\n '?',\n '#',\n 'somalia']"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "POS = 6002\n",
    "tokens = nlp(ds.text[POS])\n",
    "print(ds.target[POS], tokens)\n",
    "[token.lemma_ for token in tokens if token.lemma_ not in STOP_WORDS]\n",
    "\n",
    "# virer les lettres seules, les nombres, les '-:', les http(s)://xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['@Deeeznvtzzz']\n[]\n"
    }
   ],
   "source": [
    "POS=1003\n",
    "print([w for w in ds.text[POS].split(' ') if (w[0] == '@')])\n",
    "print([w for w in ds.text[POS].split(' ') if (w[0] == '#')])\n",
    "\n",
    "# virer les @xxx renforcer les #xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merger le keyword dans le texte ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelisation (avec ou sans preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ds = tf.data.Dataset.from_tensor_slices((ds[\"text\"].values, ds[\"target\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, target in tf_ds.take(1):\n",
    "  print(text, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ds = tf_ds.shuffle(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une instance Tokenizer\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "# Nous allons tokenizer chacun des mots en elevant les doublons via l'utilisation de set()\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in tf_ds:\n",
    "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "  vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)\n",
    "encoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, _ in tf_ds.take(1):\n",
    "  print(encoder.encode(example.numpy()))\n",
    "  print(example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodons maintenant tous les éléments d'un dataset\n",
    "def encode(text_tensor, label):\n",
    "  encoded_text = encoder.encode(text_tensor.numpy())\n",
    "  return encoded_text, label\n",
    "\n",
    "# Utilisation du fonction py_function pour encoder tout le dataset \n",
    "def encode_map_fn(text, label):\n",
    "  return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "all_encoded_data = tf_ds.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "TAKE_PROP = 0.7\n",
    "TAKE_SIZE = int(0.7 * len(ds))\n",
    "\n",
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(len(ds))\n",
    "train_data = train_data.padded_batch(16,  padded_shapes=([-1], []))\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(16, padded_shapes=([-1], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review, star in train_data.take(1):\n",
    "  print(review, star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                  # Couche d'Input Word Embedding           \n",
    "                  tf.keras.layers.Embedding(encoder.vocab_size+1, 64),\n",
    "\n",
    "                  # Couche LSTM Bidirectionnelle\n",
    "                  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "\n",
    "                  # Couche CNN\n",
    "                  tf.keras.layers.Conv1D(16, 3, activation=\"relu\"),\n",
    "                  \n",
    "                  # Nouvelle couche LSTM\n",
    "                  tf.keras.layers.LSTM(32, return_sequences=False),               \n",
    "\n",
    "                  # Couche Dense classique \n",
    "                  tf.keras.layers.Dense(64, activation='relu'),\n",
    "                  tf.keras.layers.Dense(32, activation='relu'),\n",
    "                  tf.keras.layers.Dense(16, activation='relu'),\n",
    "                  tf.keras.layers.Dense(8, activation='relu'),\n",
    "\n",
    "                  # Couche de sortie avec le nombre de neurones en sortie égale au nombre de classe avec fonction softmax\n",
    "                  tf.keras.layers.Dense(5, activation=\"softmax\")                                      \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créons un learning rate schedule pour décroitre le learning rate à mesure que nous entrainons le modèle \n",
    "initial_learning_rate = 0.0001\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "# Utilisation d'un compileur simple avec un optimiseur Adam pour le calcul de nos gradients \n",
    "optimizer= tf.keras.optimizers.Adam(\n",
    "    learning_rate = lr_schedule\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=tf.keras.metrics.SparseCategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement du modèle \n",
    "history = model.fit(train_data, epochs=40, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, _ in test_data.take(1):\n",
    "  for sents, label, pred in zip(example, _, model.predict_classes(example)):\n",
    "    print(encoder.decode(sents), label, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualisation du processus d'entrainement sur la loss function \n",
    "plt.plot(history.history[\"loss\"], color=\"b\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"r\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'entrainement sur l'accuracy \n",
    "plt.plot(history.history[\"sparse_categorical_accuracy\"], color=\"b\")\n",
    "plt.plot(history.history[\"val_sparse_categorical_accuracy\"], color=\"r\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}